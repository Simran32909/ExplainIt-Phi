# conf/config.yaml

defaults:
  - _self_

project_name: "explain-it-phi"
output_dir: "models/phi2-eli5-adapter-r32-fresh"
seed: 42

# Model config
model:
  base_model_name: "microsoft/phi-2"
  sft_max_seq_length: 512
  # Quantization
  bnb:
    load_in_4bit: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: True

  # PEFT/LoRA
  peft:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules: ["Wqkv", "fc1", "fc2"]

# Data configuration
data:
  train_file: "data/processed/train.json"
  val_file: "data/processed/validation.json"
  #num_samples_for_testing: 50000
  batch_size: 8
  num_workers: 4
  pin_memory: True

trainer:
  max_epochs: 3
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed" 
  log_every_n_steps: 25
  accumulate_grad_batches: 8
  max_grad_norm: 1.0

  # Callbacks
  early_stopping_patience: 3

  # Checkpointing
  save_top_k: 2
  monitor: "val_loss"
  mode: "min"

# Optimizer and scheduler
optimizer:
  lr: 2e-4
  weight_decay: 0.01

scheduler:
  type: "linear"
  warmup_ratio: 0.05

# W&B logger 
wandb:
  entity: "BrainLoop"
  project: "explain-it-phi-lightning" 
  log_model: "all"
  