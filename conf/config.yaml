# conf/config.yaml

defaults:
  - _self_

project_name: "explain-it-phi"
output_dir: "models/phi2-eli5-adapter-r32-fresh"
seed: 42

# Model config
model:
  base_model_name: "microsoft/phi-2"
  sft_max_seq_length: 512
  # Quantization
  bnb:
    load_in_4bit: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: True

  # PEFT/LoRA
  peft:
    r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules: ["Wqkv", "fc1", "fc2"]

# Data configuration
data:
  train_file: "data/processed/train.json"
  val_file: "data/processed/validation.json"
  num_samples_for_testing: 200000 # Set to a number for quick testing or if i dont wish to train on the full dataset
  batch_size: 8
  num_workers: 4
  pin_memory: True

trainer:
  max_epochs: 2
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed" 
  log_every_n_steps: 50
  accumulate_grad_batches: 10
  max_grad_norm: 0.3
  # Callbacks
  early_stopping_patience: 10
  # Checkpointing
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

# Optimizer and scheduler
optimizer:
  lr: 3e-5
  weight_decay: 0.002

scheduler:
  type: "cosine_with_restarts"
  warmup_ratio: 0.03

# W&B logger 
wandb:
  entity: "BrainLoop"
  project: "explain-it-phi-lightning" 
  log_model: "all"
  